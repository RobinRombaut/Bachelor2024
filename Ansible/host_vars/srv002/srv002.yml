loki_version: "latest"
loki_arch: "x86_64"
loki_http_listen_port: 3100
loki_http_listen_address: "0.0.0.0"
loki_expose_port: true
loki_download_url: "https://github.com/grafana/loki/releases/download/v{{ loki_version }}/loki-{{ loki_version }}.{{ loki_arch }}.rpm"
loki_working_path: "/var/lib/loki"
loki_ruler_alert_path: "{{ loki_working_path }}/rules/fake"
loki_target: "all"
loki_auth_enabled: false
loki_ballast_bytes: 0



global:
  # How frequently to scrape targets
  scrape_interval:     10s
  # How frequently to evaluate rules
  evaluation_interval: 10s

# Rules and alerts are read from the specified file(s)
rule_files:
    - rules.yml
groups:
  - name: AllInstances
    rules:
    - alert: InstanceDown
      # Condition for alerting
      expr: up == 0
      for: 1m
    # Annotation - additional informational labels to store more information
      annotations:
        title: 'Instance {{ $labels.instance }} down'
        description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute.'
    # Labels - additional labels to be attached to the alert
      labels:
        severity: 'critical'

# Alerting specifies settings related to the Alertmanager
alerting:
  alertmanagers:
    - static_configs:
      - targets:
        # Alertmanager's default port is 9093
        - localhost:9093

# A list of scrape configurations that specifies a set of
# targets and parameters describing how to scrape them.
scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
      - targets:
        - localhost:9090
  - job_name: 'node_exporter'
    scrape_interval: 5s
    static_configs:
      - targets:
        - localhost:9100
  - job_name: 'prom_middleware'
    scrape_interval: 5s
    static_configs:
      - targets:
        - localhost:9091





# alertmanager_web_external_url: "http://{{ ansible_host }}:9093"
# alertmanager_slack_api_url: "http://srv002.exam.lan"
# alertmanager_receivers:
#   - name: slack
#     slack_configs:
#       - send_resolved: true
#         channel: '#alerts'
# alertmanager_route:
#   group_by: ['alertname', 'cluster', 'service']
#   group_wait: 30s
#   group_interval: 5m
#   repeat_interval: 4h
#   receiver: slack
# global:
#   resolve_timeout: 5m
# route:
#   receiver: 'email-notifications'
# receivers:
# - name: 'email-notifications'
#   email_configs:
#  - to: 'rombautrobin@gmail.com'
    from: 'bachelorrobinpoc@gmail.com'
#     smarthost: 'smtp.gmail.com:587'
#     auth_username: 'bachelorrobinpoc@gmail.com'
#     auth_password: 'Robin!Bachelor!ProofOfConcept1'

# groups:
# - name: NetworkAlerts
#   rules:
#   - alert: NewNetworkConnection
#     expr: increase(network_connections_total[5m]) > 0
#     for: 1m
#     labels:
#       severity: critical
#     annotations:
#       summary: "New device connected to the network"
#       description: "A new device has been detected connecting to the network."

rhbase_firewall_allow_ports:
  - 3000/tcp
  - 9090/tcp
  - 9100/tcp
  - 3100/tcp
  - 9093/tcp

  